---
title: "Coursera Practical Machine Learning Project"
author: "Stephen DeLorenzo"
output: html_document
---

## Summary

The goal of this project is to design a machine learning model to predict how well a subject is pefoming an exercise using data collected from wearable devices.  A variety of models were tested with Random Forest emerging as a highly accurate method, reaching 99% estimated out of sample accuracy.

## Exploratory Analysis/Data Cleaning

Load needed packages and read data into R.  The RandomForest algorithm is computationally expensive, so I will prep my machine to run some of the computing in parallel across cores with the parallel and doparallel packages.
```{r loadpackages, message=FALSE}
library(caret)
library(parallel)
library(doParallel)
   train <- read.csv("C:/Users/sdeloren/Downloads/pml-training.csv")
   test <- read.csv("C:/Users/sdeloren/Downloads/pml-testing.csv")
```

This data set has many columns with blank and null records, so I will find and eliminate them.  Each transformation done to the train set must also be done to the test set for downstream prediction.

```{r removena}
 blank <- vector()
        for (i in names(train)){
                blank[i] <- sum(is.na(train[,i]))
        }
        drop <- names(blank[blank>0])
         train2 <- train[,!(names(train) %in% drop)]
         test2 <- test[,!(names(test) %in% drop)]
```

Next test for any columns that contain little to no variance and elimiate them, as they wont help the model.  Also remove other columns that dont affect the outcome like X (row number) 
```{r novar}
novar <- nearZeroVar(train2)
  train2 <- train2[,-novar]
  test2 <- test2[,-novar]
train2 <- train2[,-c(1,3:6)]
test2 <- test2[,-c(1,3:6)]
```

Sometimes predictors in a data set are highly correlated with one another which just turns out to be redundant information to the model.  Here I search for and remove those predictors.

```{r correlated}
correlations <- cor(train2[-c(1,54)])
  rm_cor <- findCorrelation(correlations,cutoff=.85,names=TRUE)
    train2 <- train2[,!(names(train2) %in% rm_cor)]
    test2 <- test2[,!(names(test2) %in% rm_cor)]
```

Before training the model, check the distribution of outcomes for potential class bias.  This data set is fairly well distributed.  We have a clean data set which is ready to test a first round model build.
```{r dist}
table(train2$classe)
```

## Model Building

Split train set further into a training/test set
```{r split}
inTrain <- createDataPartition(train2$classe,p=.7,list=FALSE)
  sub_train <- train2[inTrain,]
  sub_test <- train2[-inTrain,]
```

The Random Forest model takes a very long time to run on this dataset, making it impractical to use.  The following allows for multi core parallel processing, as well as changes caret's normal RF behavior to 10 fold cross validation.
```{r cluster}
cluster <- makeCluster(detectCores() - 1)
  registerDoParallel(cluster)
  fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
```

Then set a seed and train a Random Forest model as well asn a Linear Discriminant model for comparison.  I also attempted a Gradient Boosted model but R kept crashing.

```{r train, cashe=TRUE, message=FALSE}
set.seed(314)
        rfmod <- train(classe~.,method="rf",data=sub_train, trControl=fitControl)
set.seed(314)
        ldmod <- train(classe~.,method="lda",data=sub_train)
```

The Random Foroest model produces excellent results with 99% in sample accuracy, where the Linear Discriminate model achieves 69% in sample accuracy.  Next to predict on our test split for an out of sample accuracy estimate.  This too comes back around 99% for Random Forest, which will constitute our final model.

```{r test}
rfpred <- predict(rfmod,sub_test)
        confusionMatrix(rfpred,sub_test$classe)
```

Last we predict on the original test data set for the quiz.
```{r finalpred}
finalpred <- predict(rfmod,test2)
```

## Conclusion
A Random Forest fit works well to predict the final 'classe' from this data set.  There was some preprocessing of data to remove highly correlated predictors and junk records, but the remaining predictors served well in the model so there was not a need to further preprocess such as centering/scaling.

## Appendix
In sample Accuracy
```{r InSampleAccuracy}
rfmod
ldmod
```

Important variables
```{r varimp}
varImp(rfmod)
```